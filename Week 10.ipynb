{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Learning With Large Datasets\n",
      "\n",
      "m = training set size\n",
      "\n",
      "Problem: Summing over a very large number is inefficient\n",
      "\n",
      "Plot the learning curve of a smaller set of data to see if the the cross-validation set and the trining set approach each other\n",
      "\n",
      "If so, plot with the smaller training set"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Stochastic Gradient Descent\n",
      "\n",
      "Linear Regression with Gradient Descent:\n",
      "\n",
      "$$ h_\\theta(x) = \\sum_{j=0}^n\\theta_jx_j $$\n",
      "\n",
      "$$ J_{train}(\\theta) = \\frac{1}{2m}\\sum_{i=1}^m(h_\\theta(x^{(i)})-y^{(i)})^2 $$\n",
      "\n",
      "Efficient Algorithm:\n",
      "\n",
      "$$ cost(\\theta,(x^{(i)},y^{(i)}))=\\frac{1}{2}(h_\\theta(x^{(i)})-y^{(i)})^2 $$\n",
      "\n",
      "$$ J_{train}(\\theta)=\\frac{1}{m}\\sum_{i=1}^mcost(\\theta,(x^{(i)},y^{(i)})) $$\n",
      "\n",
      "Stochastic Gradient Descent:\n",
      "\n",
      "1.Randomly Shuffles Data\n",
      "\n",
      "2.Repeat Gradient Descent for i=1...m:\n",
      "\n",
      "$$ \\theta_j := \\theta_j - \\alpha(h_\\theta(x^{(i)}) - y^{(i)})x^{(i)}_j $$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Mini-Batch Gradient Descent\n",
      "\n",
      "Use b examples in each iteration\n",
      "\n",
      "b = mini-batch size/set-size\n",
      "\n",
      "For m/b do:\n",
      "\n",
      "$$ \\theta_j := \\theta_j - \\alpha\\frac{1}{b}\\sum_{k=i}^{i+b-1}(h_\\theta(x^{(k)})-y{(k)})x_j^{(k)} $$\n",
      "\n",
      "Have a good vectorized implementation\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Stochastic Gradient Descent Convergence\n",
      "\n",
      "During learning compute $cost(\\theta,(x^{(i)},y^{(i)}))$ before updating $\\theta$ using $(x^{(i)},y^{(i)})$\n",
      "\n",
      "For every m iterations average over m examples\n",
      "\n",
      "Change the value of m to change tot plot of the graph\n",
      "\n",
      "$$ \\alpha=-\\frac{c}{iterations+k} $$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Online Learning\n",
      "\n",
      "e.x. Shipping Services\n",
      "\n",
      "$$ p(y=1|x;\\theta) $$\n",
      "\n",
      "For each user get (x,y), j=0...n\n",
      "\n",
      "$$ \\theta_j := \\theta_j - \\alpha(h_\\theta(x) - y)x_j $$\n",
      "\n",
      "e.x product search\n",
      "\n",
      "What is the online learning algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Map-Reduce and Data Parallelism\n",
      "\n",
      "Run machines in parallel and split data into pieces\n",
      "\n",
      "take the temp variables and send them to a master server and update the parameters for $\\theta_j$\n",
      "\n",
      "Multi-core machines can split the data and send a part to each core"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}