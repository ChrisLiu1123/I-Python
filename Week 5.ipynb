{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Cost Function\n",
      "\n",
      "L = total number of layers\n",
      "\n",
      "$s_l$ = number of units in layer\n",
      "\n",
      "K = number of output nodes\n",
      "\n",
      "Logistic Regression Cost Fuction:\n",
      "\n",
      "$$ J(\\theta)=-\\frac{1}{m}\\Big[\\sum_{i=1}^my^{(i)}\\log{h_{\\theta}(x^{(i)})}+(1-y^{(i)})\\log{1-h_{\\theta}(x^{(i)})}\\Big]+\\frac{\\lambda}{2m}\\sum_{j=1}^n\\theta^2_j $$\n",
      "\n",
      "Neural Network:\n",
      "\n",
      "$$ J(\\Theta) = \\frac{1}{m}\\Big[\\sum_{i=1}^m\\sum_{k=1}^Ky^{(i)}_k\\log({h_{\\Theta}(x^{(i)})})_k+(1-y^{(i)}_k)\\log({1-h_{\\theta}(x^{(i)})_k})\\Big]+\\frac{\\lambda}{2m}\\sum_{l=1}^{L-1}\\sum_{i=1}^{s_l}\\sum_{j=1}^{s_l+1}(\\Theta_{ji}^{(l)})^2 $$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Backpropagation Algorithm\n",
      "\n",
      "Error of node j in layer l:\n",
      "\n",
      "$$ \\delta_j^{(l)} = a_j^{(l)} - y_j $$\n",
      "\n",
      "$$ a_j^{(l)} = (h_{\\theta}^{(x)})_j $$\n",
      "\n",
      "$$ \\delta^{(l)} = (\\Theta^{(l)})^T\\delta^{(l+1)}g'(z^{(l)}) $$\n",
      "\n",
      "Algorithm:\n",
      "\n",
      "$$ \\Delta_{ij}^{(l)} = 0 $$\n",
      "\n",
      "$$ \\Delta_{ij}^{(l)} := \\Delta_{ij}^{(l)} + a_j^{(l)}\\delta_i^{(l+1)} $$\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How to do the partial derivative symbol for gradient descent?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": []
    }
   ],
   "metadata": {}
  }
 ]
}